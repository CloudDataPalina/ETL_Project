{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88d7e01a-8025-4fdd-a53e-bd6addbca2e6",
   "metadata": {},
   "source": [
    "# 🚗 Car Prices ETL Pipeline\n",
    "\n",
    "## 🎯 Objective\n",
    "\n",
    "This mini-project implements a simple yet functional **ETL process** (Extract, Transform, Load) — a key component of a **Data Engineer’s** work.\n",
    "\n",
    "- The source data on used car prices is provided in various formats: **CSV**, **JSON**, and **XML**.  \n",
    "- The task is to extract the data, convert it into a unified tabular format, and prepare it for saving in a final CSV file.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Process Overview\n",
    "\n",
    "- **Extract**  \n",
    "  Automatic collection of data from all `.csv`, `.json`, and `.xml` files in the working directory.\n",
    "\n",
    "- **Transform**  \n",
    "  Data standardization: applying proper data types and rounding price values.\n",
    "\n",
    "- **Load**  \n",
    "  Saving the final DataFrame into `transformed_data.csv`.  \n",
    "  Logging each processing step into `log_file.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Practical Relevance\n",
    "\n",
    "This project demonstrates how to work with **heterogeneous data sources**, apply a **functional approach**, and use **logging** — all of which are crucial aspects of real-world ETL pipelines.\n",
    "\n",
    "It can serve as a **basic template for future projects** involving external data sources.\n",
    "\n",
    "---\n",
    "\n",
    "Next comes the step-by-step implementation: importing libraries, extracting, transforming, and loading the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9b315-24b1-4075-b776-d78e2587df34",
   "metadata": {},
   "source": [
    "## Step 1. Importing Libraries and Setting Up Variables\n",
    "\n",
    "To implement the ETL process in this project, we use the following standard and third-party Python libraries:\n",
    "\n",
    "- **pandas** — a library for working with tabular data. It is used for reading CSV and JSON files, transforming data, and creating the final DataFrame.\n",
    "- **glob** — a module for finding all files in a directory that match a specific pattern (e.g., `*.csv`, `*.json`, `*.xml`). It enables automatic discovery and processing of data sources.\n",
    "- **xml.etree.ElementTree** — a built-in module for parsing XML files. It allows extracting data from hierarchical XML structures.\n",
    "- **datetime** — a module for handling date and time. It is used to add timestamps to the log file that tracks each ETL step.\n",
    "\n",
    "All of these libraries are included in Python’s standard distribution, except for `pandas`, which can be installed using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0b83c72-4c76-459d-ace9-b920ce792672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\palina\\it\\programs\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\palina\\it\\programs\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\palina\\it\\programs\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\palina\\it\\programs\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\palina\\it\\programs\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\palina\\it\\programs\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98c069-4624-4d17-98ef-28a2984c6613",
   "metadata": {},
   "source": [
    "The next step is to import the required libraries and set up the global variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248f5740-c9ff-40cb-b401-d53f2c8fb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                   # Working with tabular data (DataFrame)\n",
    "import glob                           # Searching for files matching a pattern (*.csv, *.json, *.xml)\n",
    "import xml.etree.ElementTree as ET    # Working with XML structure\n",
    "from datetime import datetime         # Adding timestamps to the log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef42851-12b4-49fe-9b7a-b36d94fd8ba3",
   "metadata": {},
   "source": [
    "To ensure the correct execution of the ETL script, two global file paths are defined:\n",
    "\n",
    "- `log_file.txt` — a file for writing execution logs. It will store timestamps and each ETL stage in sequence.\n",
    "- `transformed_data.csv` — the final file where the combined and transformed dataset will be saved, ready for database loading or further analysis.\n",
    "\n",
    "These variables are used in all key functions of the project: logging, data loading, and saving results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b08ef7e-e1b5-4948-883d-d39bd02ea2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VARIABLES ===\n",
    "log_file = \"output/log_file.txt\"             # File to log the progress of the ETL process\n",
    "target_file = \"output/\n",
    "transformed_data.csv\"  # Final CSV file with the transformed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657efb5d-2315-44d9-996b-5a8e9600434e",
   "metadata": {},
   "source": [
    "## Step 2. Logging Function\n",
    "\n",
    "To track the progress of the ETL process, the project includes a `log_progress()` function. It writes messages to the `log_file.txt`, adding a timestamp to each entry.\n",
    "\n",
    "This approach helps to:\n",
    "\n",
    "- Verify which steps have already been executed  \n",
    "- Debug errors or unexpected behavior  \n",
    "- Maintain an execution history (e.g., when running the script on a schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6099cb52-7cf2-473d-b9d9-71effb491a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    timestamp_format = '%Y-%b-%d-%H:%M:%S'      # Timestamp format, e.g. 2025-Jul-26-19:45:04\n",
    "    now = datetime.now()                        # Get the current time\n",
    "    timestamp = now.strftime(timestamp_format)  # Format the time as a string\n",
    "    with open(log_file, \"a\") as f:              # Open the log file in append mode\n",
    "        f.write(f\"{timestamp},{message}\\n\")     # Write the timestamp and message to the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcbb2cd-4d8a-4918-aa08-45aa48f8f1d9",
   "metadata": {},
   "source": [
    "## Step 3. Data Extraction\n",
    "\n",
    "At this stage, data is automatically loaded from files in ***CSV***, ***JSON***, and ***XML*** formats.  \n",
    "A separate function is used for each format, returning a pandas ***DataFrame*** structure.\n",
    "\n",
    "This modular approach improves code readability and makes it easy to adapt solutions for other projects or data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff590302-41a5-4d77-afdc-10a54769e414",
   "metadata": {},
   "source": [
    "### 3.1. Extracting Data from CSV\n",
    "\n",
    "The first function, `extract_from_csv()`, is responsible for extracting data from **CSV** (comma-separated values) files.\n",
    "\n",
    "It uses `pandas.read_csv()` to load the file content into a **DataFrame** — a tabular structure that is ideal for further processing and merging of data.\n",
    "\n",
    "This approach makes it easy to scale the process across multiple files by automating data extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6a5b5a-d8e2-4211-8d16-cf19a0553cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_to_process):\n",
    "    return pd.read_csv(file_to_process)  # Reading a CSV file into a DataFrame using pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a890f9e-0914-4dc6-a9bd-90e5b1b71e91",
   "metadata": {},
   "source": [
    "### 3.2. Extracting Data from JSON\n",
    "\n",
    "The `extract_from_json()` function is used to process files in the **JSON Lines** format (one JSON object per line).\n",
    "\n",
    "By using the `lines=True` parameter, the `pandas.read_json()` method correctly reads the data line by line into individual records. This format is commonly used for storing logs, data streams, and system exports.\n",
    "\n",
    "The function returns a **DataFrame**, similar to the CSV data extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7e7e59-347b-4c7a-84f0-e4498fa3369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_json(file_to_process):\n",
    "    # Reads a JSON file line by line in JSON Lines format (each line is a separate JSON object)\n",
    "    return pd.read_json(file_to_process, lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b25cb2-6730-4a4d-b643-5e1770f3b352",
   "metadata": {},
   "source": [
    "### 3.3. Extracting Data from XML\n",
    "\n",
    "The `extract_from_xml()` function handles reading and parsing data from **XML files**.  \n",
    "Unlike CSV and JSON, XML requires a step-by-step traversal of elements.\n",
    "\n",
    "The project uses the `ElementTree` module from Python's standard library to:\n",
    "\n",
    "- parse the XML file;\n",
    "- get the root element;\n",
    "- extract values from the `<car_model>`, `<year_of_manufacture>`, `<price>`, and `<fuel>` tags inside each item.\n",
    "\n",
    "Each data row is added to a predefined **DataFrame** with the necessary columns.  \n",
    "This approach simplifies working with XML and makes the code easier to reuse for more complex XML structures in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ba43e4-c4a8-473a-87f8-f198cf9625d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_xml(file_to_process):\n",
    "    # Create an empty DataFrame with predefined columns and data types\n",
    "    # Using pd.Series(dtype=...) to explicitly define the structure\n",
    "    dataframe = pd.DataFrame({\n",
    "        \"car_model\": pd.Series(dtype=\"str\"),            # string\n",
    "        \"year_of_manufacture\": pd.Series(dtype=\"int\"),  # integer\n",
    "        \"price\": pd.Series(dtype=\"float\"),              # float\n",
    "        \"fuel\": pd.Series(dtype=\"str\")                  # string\n",
    "    })\n",
    "\n",
    "    # Load and parse the XML file\n",
    "    tree = ET.parse(file_to_process)    # create XML tree object\n",
    "    root = tree.getroot()               # get root element of the XML structure\n",
    "\n",
    "    # Iterate over each item (e.g., <car>) within the root\n",
    "    for i in root:\n",
    "        # Extract values from sub-elements of the current element\n",
    "        car_model = i.find(\"car_model\").text\n",
    "        year_of_manufacture = int(i.find(\"year_of_manufacture\").text)\n",
    "        price = float(i.find(\"price\").text)\n",
    "        fuel = i.find(\"fuel\").text\n",
    "\n",
    "        # Build a dictionary from the extracted values\n",
    "        row = {\n",
    "            \"car_model\": car_model,\n",
    "            \"year_of_manufacture\": year_of_manufacture,\n",
    "            \"price\": price,\n",
    "            \"fuel\": fuel\n",
    "        }\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        dataframe.loc[len(dataframe)] = row\n",
    "\n",
    "    # Return the final DataFrame with data extracted from XML\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e299f38-bfdb-4d18-b54e-d7788d4fd83b",
   "metadata": {},
   "source": [
    "## Step 4. Merging All Data\n",
    "\n",
    "The `extract()` function consolidates data extraction from all supported formats: **CSV**, **JSON**, and **XML**. It also:\n",
    "\n",
    "- creates an empty typed DataFrame with the required columns;\n",
    "- uses the `glob` module to iterate through all files in the working directory;\n",
    "- calls the appropriate function depending on the file format;\n",
    "- merges the extracted data into a single final DataFrame;\n",
    "- skips the final CSV file from processing to avoid recursion.\n",
    "\n",
    "This approach allows automatic processing of entire folders with mixed file types without relying on specific filenames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11afe0e7-11ed-4eea-a90a-02ce697a87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    # Create an empty DataFrame with predefined columns and data types\n",
    "    extracted_data = pd.DataFrame({\n",
    "        \"car_model\": pd.Series(dtype=\"str\"),               # Car brand/model\n",
    "        \"year_of_manufacture\": pd.Series(dtype=\"int\"),     # Year of manufacture\n",
    "        \"price\": pd.Series(dtype=\"float\"),                 # Price\n",
    "        \"fuel\": pd.Series(dtype=\"str\")                     # Fuel type\n",
    "    })\n",
    "\n",
    "    # === Extract data from CSV files ===\n",
    "    for csvfile in glob.glob(\"datasource/*.csv\"):                     # Loop through all .csv files in the current directory\n",
    "        if csvfile != target_file:                         # Skip the final output file to avoid recursive processing\n",
    "            new_data = extract_from_csv(csvfile)           # Extract data using extract_from_csv()\n",
    "            if not new_data.empty:                         # Only process if the result is not empty\n",
    "                # Append to the main DataFrame\n",
    "                # ignore_index=True resets row indices (0 to N-1) after concatenation\n",
    "                extracted_data = pd.concat([extracted_data, new_data], ignore_index=True)\n",
    "\n",
    "    # === Extract data from JSON files ===\n",
    "    for jsonfile in glob.glob(\"datasource/*.json\"):                   # Loop through all .json files\n",
    "        new_data = extract_from_json(jsonfile)             # Extract data from file\n",
    "        if not new_data.empty:                           # Skip empty results\n",
    "            extracted_data = pd.concat([extracted_data, new_data], ignore_index=True)\n",
    "\n",
    "    # === Extract data from XML files ===\n",
    "    for xmlfile in glob.glob(\"datasource/*.xml\"):                     # Loop through all .xml files\n",
    "        new_data = extract_from_xml(xmlfile)               # Extract data from file\n",
    "        if not new_data.empty:                           # Skip empty results\n",
    "            extracted_data = pd.concat([extracted_data, new_data], ignore_index=True)\n",
    "\n",
    "    return extracted_data                                   # Return the merged DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979400cd-efe3-42cf-89de-ecead8210a55",
   "metadata": {},
   "source": [
    "## Step 5. Data Transformation\n",
    "\n",
    "The `transform()` function performs a basic transformation:  \n",
    "- **rounding prices to two decimal places** in the `price` column.\n",
    "\n",
    "This step is important because it:\n",
    "\n",
    "- removes unnecessary precision (e.g., values with 10 decimal places);\n",
    "- converts prices into a format commonly used in reports, databases, and user interfaces.\n",
    "\n",
    "The function returns the updated DataFrame, ready for saving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da28b46-621b-4933-a7b8-ced372dc420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    # Transform the 'price' column by rounding values to two decimal places\n",
    "    data['price'] = round(data['price'], 2)\n",
    "    return data  # Return the transformed DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8704c-a810-44a8-b0a1-73f5bb67fea6",
   "metadata": {},
   "source": [
    "## Step 6. Saving the Result\n",
    "\n",
    "The final stage of the ETL pipeline is saving the transformed data.\n",
    "\n",
    "The `load_data()` function exports the final **DataFrame** to a CSV file specified by the `target_file` variable.  \n",
    "The parameter `index=False` ensures that the DataFrame index is not saved as a separate column — which is useful when importing the data into a database.\n",
    "\n",
    "The resulting `transformed_data.csv` file can be used as a ready-to-go dataset for analysis, visualization, or storage in a data warehouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475d96bc-9d4c-4a53-97be-265f0ae33791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(target_file, transformed_data):\n",
    "    # Save the transformed data to a CSV file without including the index\n",
    "    transformed_data.to_csv(target_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde5617-8305-45e0-bc3c-bc34a5687abf",
   "metadata": {},
   "source": [
    "## Step 7. Running the ETL Pipeline and Logging the Execution Flow\n",
    "\n",
    "The full ETL cycle is executed below — from logging the start to saving the final result:\n",
    "\n",
    "1. **Initialization**: a \"start\" entry is written to the log\n",
    "2. **Extraction**: data is loaded from all CSV, JSON, and XML files\n",
    "3. **Transformation**: price values are rounded to two decimal places\n",
    "4. **Loading**: the consolidated dataset is saved to a CSV file\n",
    "5. **Logging**: each step is recorded with a timestamp in `log_file.txt`\n",
    "\n",
    "Each stage is not only executed but also documented — which is important for reproducibility, debugging, and future automation.\n",
    "\n",
    "The `print()` output is used to visually inspect the transformed data before saving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119592c7-c9b5-4407-a2f0-544b3a7ddb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Data:\n",
      "        car_model  year_of_manufacture     price    fuel\n",
      "0            ritz                 2014   5000.00  Petrol\n",
      "1             sx4                 2013   7089.55  Diesel\n",
      "2            ciaz                 2017  10820.90  Petrol\n",
      "3         wagon r                 2011   4253.73  Petrol\n",
      "4           swift                 2014   6865.67  Diesel\n",
      "..            ...                  ...       ...     ...\n",
      "85          camry                 2006   3731.34  Petrol\n",
      "86   land cruiser                 2010  52238.81  Diesel\n",
      "87  corolla altis                 2012   8805.97  Petrol\n",
      "88     etios liva                 2013   5149.25  Petrol\n",
      "89        etios g                 2014   7089.55  Petrol\n",
      "\n",
      "[90 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# === RUNNING THE ETL PIPELINE ===\n",
    "log_progress(\"ETL Job Started\")\n",
    "\n",
    "# Extract phase\n",
    "log_progress(\"Extract phase Started\")\n",
    "extracted_data = extract()\n",
    "log_progress(\"Extract phase Ended\")\n",
    "\n",
    "# Transform phase\n",
    "log_progress(\"Transform phase Started\")\n",
    "transformed_data = transform(extracted_data)\n",
    "print(\"Transformed Data:\")\n",
    "print(transformed_data)\n",
    "log_progress(\"Transform phase Ended\")\n",
    "\n",
    "# Load phase\n",
    "log_progress(\"Load phase Started\")\n",
    "load_data(target_file, transformed_data)\n",
    "log_progress(\"Load phase Ended\")\n",
    "\n",
    "# Completion\n",
    "log_progress(\"ETL Job Ended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d25f4-1745-450a-ab93-05fdf54ed044",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "This mini-project implemented a complete ETL process using Python:\n",
    "\n",
    "🔹 **Extract:** data successfully loaded from CSV, JSON, and XML files  \n",
    "🔹 **Transform:** price values rounded to two decimal places  \n",
    "🔹 **Load:** the combined result saved to `transformed_data.csv`  \n",
    "📝 The entire execution process was logged in the `log_file.txt` file\n",
    "\n",
    "This project demonstrates how to automate the processing of various data formats and prepare them for further analytics or data warehousing.\n",
    "\n",
    "---\n",
    "\n",
    "**This ETL project** is a great starting point for building more advanced ETL pipelines with other data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ae1e7-4b2c-4593-9eb3-4f6cab8d4400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
